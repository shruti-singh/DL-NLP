{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer acts as a lookup table that maps the integer indices to vectors. When we create an embedding layer, the weights are randomly initialized and then adjusted via backpropogation during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters to the Embedding Layer are input_dim, output_dim, and input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *input_dim* is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*output_dim* is the size of the vector space in which the words are embedded, i.e. the dimension of the vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *input_length* is the length of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a sequence of integers to the embedding layer returns the floating point vector which is mapped to the integer sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OVERLOADABLE_OPERATORS', '__abs__', '__add__', '__and__', '__array_priority__', '__bool__', '__class__', '__copy__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__invert__', '__iter__', '__le__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_as_node_def_input', '_as_tf_output', '_c_api_shape', '_consumers', '_dtype', '_get_input_ops_without_shapes', '_id', '_keras_mask', '_op', '_override_operator', '_rank', '_shape', '_shape_as_list', '_shape_tuple', '_shape_val', '_tf_api_names', '_tf_api_names_v1', '_uses_learning_phase', '_value_index', 'consumers', 'device', 'dtype', 'eval', 'get_shape', 'graph', 'name', 'op', 'set_shape', 'shape', 'value_index']\n",
      "[[ 0.01936573  0.04533804  0.01081066  0.04869247 -0.01155156]\n",
      " [ 0.00752498  0.01560744 -0.04541853 -0.03889602 -0.01744169]\n",
      " [ 0.01378195  0.03551667  0.04126629 -0.04587668  0.02179531]\n",
      " [-0.04301316 -0.01809986 -0.0462363  -0.04270166 -0.02698705]]\n"
     ]
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,4,7,9]))\n",
    "print(dir(result))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(result.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a 2D tensor of integers to the Embedding layer, of shape **(number_of_samples, sequence_length)**. Each entry is a sequence of integers. The output is a 3D floating point tensor, which has shape **(number_of_samples, sequence_length, embedding_dim)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[1,2,4],[2,3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.0167932   0.04114074  0.04917233 -0.04275043  0.00532372]\n",
      "  [-0.005925   -0.03394087 -0.01473413  0.04891843  0.0375384 ]\n",
      "  [ 0.03977672 -0.01337009 -0.00111664 -0.03263972  0.01183114]]\n",
      "\n",
      " [[-0.005925   -0.03394087 -0.01473413  0.04891843  0.0375384 ]\n",
      "  [ 0.00768874 -0.00831401 -0.03754973  0.03026772 -0.04218663]\n",
      "  [ 0.03977672 -0.01337009 -0.00111664 -0.03263972  0.01183114]]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    res = result.eval()\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
