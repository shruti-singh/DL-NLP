{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "yuAY8j58zEzc",
    "outputId": "5dadc6e6-6e2c-4726-f774-a5e51abefd88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices()\n",
    "print(*physical_devices, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU0 is busy, use GPU1\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(physical_devices[2], 'XLA_GPU')\n",
    "    tf.config.experimental.get_visible_devices()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "Variable placed on device:  /job:localhost/replica:0/task:0/device:CPU:0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1\n",
      "Is GPU available:  False\n"
     ]
    }
   ],
   "source": [
    "test_var = tf.constant([1.0,2.0,3.0], name='test_var')\n",
    "print(test_var)\n",
    "print(\"Variable placed on device: \", test_var.device)\n",
    "\n",
    "with tf.device('/device:XLA_GPU:1'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    print(a.device)\n",
    "    print(b.device)\n",
    "\n",
    "# ?????\n",
    "print(\"Is GPU available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')\n",
      "LogicalDevice(name='/job:localhost/replica:0/task:0/device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "LogicalDevice(name='/job:localhost/replica:0/task:0/device:XLA_GPU:1', device_type='XLA_GPU')\n",
      "LogicalDevice(name='/job:localhost/replica:0/task:0/device:XLA_CPU:0', device_type='XLA_CPU')\n"
     ]
    }
   ],
   "source": [
    "# Return a list of logical devices created by runtime. Logical devices may correspond to physical devices or remote devices in the cluster. \n",
    "# Operations and tensors may be placed on these devices by using the name of the LogicalDevice.\n",
    "logical_devices = tf.config.experimental.list_logical_devices()\n",
    "print(*logical_devices, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'get_experimental_options', 'get_jit', 'set_experimental_options', 'set_jit']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(dir(tf.config.optimizer))\n",
    "print(tf.config.optimizer.get_jit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpuA8y0UzKEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9p0ds9YJ5Jk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version;\n",
    "print(version.VERSION)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3h8tqo9iziuX",
    "outputId": "d7608e6d-3962-4643-e383-e5e857c06fd6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "data_dir_path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-L0sjGzzrjf"
   },
   "outputs": [],
   "source": [
    "train_validation_split = tfds.Split.TRAIN.subsplit([7, 3])\n",
    "((train_dataset, validation_dataset), test_dataset), info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True, data_dir=data_dir_path, download=False, split=(train_validation_split, tfds.Split.TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "2AtD0fkezys5",
    "outputId": "96597a19-78fc-4612-b106-505eae2ea4b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info features:  FeaturesDict({\n",
      "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
      "})\n",
      "\n",
      " Vocabulary size:  8185\n"
     ]
    }
   ],
   "source": [
    "print(\"info features: \", info.features)\n",
    "encoder = info.features[\"text\"].encoder\n",
    "print(\"\\n Vocabulary size: \", encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_EIgZl7z81j"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "# output_shapes returns the shape of each component of an element of this dataset.\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "\n",
    "validation_dataset = validation_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(validation_dataset))\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkvQyeeI3d_9"
   },
   "source": [
    "The base class [RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN?version=stable) for recurrent layers inherits from class keras.layers.Layer. Each RNN cell isntance must have the following:\n",
    "\n",
    "*   state_size attribute\n",
    "*   output_size attriute\n",
    "*   call(input_at_t, state_at_t) method, which return output_at_t and state_at_t_plus_1.\n",
    "*   get_initial_state(inputs=None, batch_size=None, dtype=None) method that creates a tensor meant to be fed to call() as the initial state, if the user didn't specify any initial state via other means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFDUmUUJ2Di-"
   },
   "outputs": [],
   "source": [
    "class CustomRNNCell(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        \n",
    "        if 'teacher_forcing' in kwargs:\n",
    "            setattr(self, 'teacher_forcing', kwargs['teacher_forcing'])\n",
    "        else:\n",
    "            setattr(self, 'teacher_forcing', False)\n",
    "        \n",
    "        super(CustomRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # add_weight is from base_layer class, used to add a new variable to the layer\n",
    "        \n",
    "        # kernel initializer, weight matrix used for the linear transformation of the inputs\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer='uniform', name='kernel')\n",
    "\n",
    "        # recurrent initializer, weight matrix used for the linear transformation of the recurrent state.\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), initializer='uniform', name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # if self.teacher_forcing:\n",
    "            # TODO: Figure out how to incorporate decay. Send the true label to next hidden state. \n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs, self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        # For a simple RNN, the output_at_t and hidden_state_at_t_plus_1 is same.\n",
    "        return output, [output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "7LzzzLfp5Pua",
    "outputId": "cf8083d4-faf9-4b67-eaed-acadbc2f5f4a"
   },
   "outputs": [],
   "source": [
    "def generate_model(use_dropout=False):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=64))\n",
    "    model.add(tf.keras.layers.RNN([CustomRNNCell(8)]))\n",
    "    if use_dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgVflgpZ5cGF"
   },
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def binary_crossentropy(y_true, y_pred, from_logits=False):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        def get_epsilon():\n",
    "            # epsilon_value = 1e-7\n",
    "            return tf.keras.backend.epsilon()\n",
    "\n",
    "        if not from_logits:\n",
    "            if y_pred.op.type == \"Sigmoid\":\n",
    "                tf.reduce_mean(tf.math.add(tf.math.negative(tf.math.multiply(y_pred, y_true)), tf.math.log(tf.math.add(1., tf.math.exp(y_pred)))))\n",
    "            epsilon = get_epsilon()\n",
    "            clipped_y_pred = tf.clip_by_value(y_pred, clip_value_min=epsilon, clip_value_max=(1.-epsilon))\n",
    "            bce = tf.math.multiply(y_true, tf.math.log(tf.math.add(clipped_y_pred, epsilon)))\n",
    "            temp = tf.math.multiply(tf.math.subtract(1., y_true), tf.math.log(tf.math.add(epsilon, tf.math.subtract(1., clipped_y_pred))))\n",
    "            return tf.math.negative(tf.reduce_mean(tf.math.add(bce, temp)))\n",
    "        else:\n",
    "            # - x * z + log(1 + exp(x)), x = logits, z = labels\n",
    "            return tf.reduce_mean(tf.math.add(tf.math.negative(tf.math.multiply(y_pred, y_true)), tf.math.log(tf.math.add(1., tf.math.exp(y_pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYhWpg6MMW30"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(loss=LossFunction.binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./tf_logs/rnn_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "m1RuTehe7MjX",
    "outputId": "d45b1bc6-535b-4c95-d1f6-302b94737728"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, validation_dataset, epochs=20):\n",
    "#     callbacks = [\n",
    "#         # Write TensorBoard logs to `./tf_logs/rnn` directory\n",
    "#         tf.keras.callbacks.TensorBoard(log_dir='./tf_logs/rnn_1', histogram_freq=10, write_graph=True)\n",
    "#     ]\n",
    "    callbacks = []\n",
    "    \n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=validation_dataset, validation_steps=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gg-SzZWkAHKc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 8)                 576       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 524,577\n",
      "Trainable params: 524,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = generate_model()\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 422s 1s/step - loss: 0.6932 - accuracy: 0.4963 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "352/352 [==============================] - 342s 973ms/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 3/20\n",
      "352/352 [==============================] - 341s 970ms/step - loss: 0.6929 - accuracy: 0.5016 - val_loss: 0.6920 - val_accuracy: 0.5125\n",
      "Epoch 4/20\n",
      "352/352 [==============================] - 337s 958ms/step - loss: 0.6893 - accuracy: 0.5025 - val_loss: 0.6968 - val_accuracy: 0.5094\n",
      "Epoch 5/20\n",
      "352/352 [==============================] - 335s 952ms/step - loss: 0.6852 - accuracy: 0.5049 - val_loss: 0.6915 - val_accuracy: 0.4969\n",
      "Epoch 6/20\n",
      "352/352 [==============================] - 346s 982ms/step - loss: 0.6831 - accuracy: 0.5053 - val_loss: 0.6954 - val_accuracy: 0.5125\n",
      "Epoch 7/20\n",
      "352/352 [==============================] - 344s 978ms/step - loss: 0.6820 - accuracy: 0.5041 - val_loss: 0.6984 - val_accuracy: 0.4969\n",
      "Epoch 8/20\n",
      "352/352 [==============================] - 336s 954ms/step - loss: 0.6829 - accuracy: 0.5050 - val_loss: 0.7095 - val_accuracy: 0.5094\n",
      "Epoch 9/20\n",
      "352/352 [==============================] - 341s 969ms/step - loss: 0.6828 - accuracy: 0.5034 - val_loss: 0.7028 - val_accuracy: 0.4969\n",
      "Epoch 10/20\n",
      "352/352 [==============================] - 344s 977ms/step - loss: 0.6822 - accuracy: 0.5051 - val_loss: 0.7039 - val_accuracy: 0.4969\n",
      "Epoch 11/20\n",
      "352/352 [==============================] - 344s 976ms/step - loss: 0.6820 - accuracy: 0.5035 - val_loss: 0.7079 - val_accuracy: 0.4938\n",
      "Epoch 12/20\n",
      "352/352 [==============================] - 344s 977ms/step - loss: 0.6820 - accuracy: 0.5066 - val_loss: 0.7075 - val_accuracy: 0.4938\n",
      "Epoch 13/20\n",
      "352/352 [==============================] - 344s 977ms/step - loss: 0.6809 - accuracy: 0.5077 - val_loss: 0.7135 - val_accuracy: 0.4938\n",
      "Epoch 14/20\n",
      "352/352 [==============================] - 341s 970ms/step - loss: 0.6817 - accuracy: 0.5062 - val_loss: 0.7181 - val_accuracy: 0.5094\n",
      "Epoch 15/20\n",
      "352/352 [==============================] - 341s 970ms/step - loss: 0.6821 - accuracy: 0.5061 - val_loss: 0.7170 - val_accuracy: 0.4938\n",
      "Epoch 16/20\n",
      "352/352 [==============================] - 343s 974ms/step - loss: 0.6814 - accuracy: 0.5068 - val_loss: 0.7294 - val_accuracy: 0.5094\n",
      "Epoch 17/20\n",
      "352/352 [==============================] - 339s 963ms/step - loss: 0.6811 - accuracy: 0.5053 - val_loss: 0.7443 - val_accuracy: 0.4938\n",
      "Epoch 18/20\n",
      "352/352 [==============================] - 346s 984ms/step - loss: 0.6822 - accuracy: 0.5071 - val_loss: 0.7356 - val_accuracy: 0.4938\n",
      "Epoch 19/20\n",
      "352/352 [==============================] - 342s 972ms/step - loss: 0.6818 - accuracy: 0.5092 - val_loss: 0.7368 - val_accuracy: 0.5094\n",
      "Epoch 20/20\n",
      "352/352 [==============================] - 342s 971ms/step - loss: 0.6815 - accuracy: 0.5092 - val_loss: 0.7296 - val_accuracy: 0.4938\n",
      "CPU times: user 4h 44min 43s, sys: 31min 10s, total: 5h 15min 54s\n",
      "Wall time: 1h 55min 16s\n"
     ]
    }
   ],
   "source": [
    "%time train_model(model, train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    391/Unknown - 128s 328ms/step - loss: 0.7559 - accuracy: 0.5006"
     ]
    }
   ],
   "source": [
    "rnn_test_loss, rnn_test_acc = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('rnn_classification', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "rnn_1 (RNN)                  (None, 8)                 576       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 524,577\n",
      "Trainable params: 524,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_model = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = compile_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6932723, 0.40625]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = train_dataset.__iter__().next()\n",
    "\n",
    "# This initializes the variables used by the optimizers, as well as any stateful metric variables\n",
    "# The optimizer state is preserved as well, so we can resume training where we left off\n",
    "new_model.train_on_batch(p[0], p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f935de62e80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state of the old model\n",
    "new_model.load_weights('rnn_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the model state has been preserved\n",
    "old_predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = new_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(old_predictions, new_predictions, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using hyperparam tuning and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout, dense layer size, 2 custom recurrent layers with teacher forcing, early stopping after 4 epochs and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Clear logs from previous runs \n",
    "# rm -rf ./tf_logs/rnn_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg:\n",
    "    @staticmethod\n",
    "    def l1_reg(weight_matrix):\n",
    "        return 0.01 * K.sum(K.abs(weight_matrix))\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_reg(weight_matrix):\n",
    "        return 0.01 * 0.5 * K.sum(K.square(weight_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerRNN:\n",
    "    \n",
    "    def __init__(self, teacher_forcing=False):\n",
    "        # number of units in 1st and 2nd recurrent layer, and the next dense layer\n",
    "        self.num_units_l1 = hp.HParam('num_units_l1', hp.Discrete([16, 32, 64]))\n",
    "        self.num_units_l2 = hp.HParam('num_units_l2', hp.Discrete([16, 32, 64]))\n",
    "        self.num_units_l3 = hp.HParam('num_units_l3', hp.Discrete([20, 30, 40]))\n",
    "        self.dropout = hp.HParam('dropout', hp.Discrete([0.3, 0.4]))\n",
    "        \n",
    "#         self.learning_rate = hp.HParam('learning_rate', hp.RealInterval(0.01, 0.5))\n",
    "        self.optimizer = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "        \n",
    "        self.hparams = {self.optimizer: self.optimizer, self.num_units_l1: self.num_units_l1, self.num_units_l2: self.num_units_l2, self.num_units_l3: self.num_units_l3, self.dropout: self.dropout}\n",
    "        \n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        METRIC_ACCURACY = 'accuracy'\n",
    "        \n",
    "        self.init_timestamp = time()\n",
    "        \n",
    "        self.log_dir = \"./tf_logs/rnn_classification_\" + str(self.init_timestamp) +\"/\"\n",
    "        with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "            hp.hparams_config(hparams=[self.optimizer, self.num_units_l1, self.num_units_l2, self.num_units_l3, self.dropout], metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def loss_function(self, y_true, y_pred):\n",
    "        r = 0.0\n",
    "        for w in self.model.trainable_weights:\n",
    "            r += Reg.l2_reg(w)\n",
    "        l = LossFunction.binary_crossentropy(y_true, y_pred) + r\n",
    "        return l\n",
    "    \n",
    "#     def loss_function(self, y_true, y_pred):\n",
    "#         return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    def generate_model(self, params):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=64))\n",
    "        self.model.add(tf.keras.layers.RNN([CustomRNNCell(params[self.num_units_l1]), CustomRNNCell(params[self.num_units_l2])]))\n",
    "        self.model.add(tf.keras.layers.Dropout(params[self.dropout]))\n",
    "        self.model.add(tf.keras.layers.Dense(params[self.num_units_l3], activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        print(self.model.summary())\n",
    "        return self.model\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self):\n",
    "        cp = time()\n",
    "        model.save_weights(self.logdir + '/saved_models/model_' + cp, save_format='tf')\n",
    "        return\n",
    "    \n",
    "    def compile_model(self, loss_function, optimizer):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "        return self.model\n",
    "    \n",
    "    def train_model(self, hparams, train_data, cross_validation_data, run_index):\n",
    "        self.generate_model(hparams)\n",
    "        self.compile_model(self.loss_function, hparams[self.optimizer])\n",
    "#         self.compile_model(self.loss_function, self.optimizer)\n",
    "        \n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4),\n",
    "            # logging train and validation accuracy each epoch\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=self.log_dir + run_index),\n",
    "            # checkpoint\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=, save_weights_only=True, save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        self.model.fit(train_data, epochs=10, validation_data=cross_validation_data, callbacks=callbacks, verbose=1)\n",
    "        _, accuracy = self.model.evaluate(cross_validation_data)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def run(self, run_dir, hparams, train_data, cross_validation_data):\n",
    "        K.clear_session()\n",
    "        run_index = run_dir.split(\"-\")[1]\n",
    "        with tf.summary.create_file_writer(run_dir).as_default():\n",
    "            # record the values used in this trial\n",
    "            hp.hparams(hparams)\n",
    "            acc = self.train_model(hparams, train_dataset, validation_dataset, run_index)\n",
    "            tf.summary.scalar('accuracy', acc, step=int(run_index))\n",
    "        return acc\n",
    "    \n",
    "    def random_search(self, train, cross_val, seed):\n",
    "        rng = random.Random(seed)\n",
    "        total_points_explored = 10\n",
    "        \n",
    "        acc_params = []\n",
    "        \n",
    "        for session_index in range(total_points_explored):\n",
    "            hparams = {h: h.domain.sample_uniform(rng) for h in self.hparams}\n",
    "            run_name = \"run-%d\" % session_index\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            acc = self.run(self.log_dir + \"tune/\" + run_name, hparams, train, cross_val)\n",
    "            session_index += 1\n",
    "            acc_params.append((acc, hparams))\n",
    "        \n",
    "        return total_points_explored, acc_params\n",
    "    \n",
    "    def setup_model(self, params):\n",
    "        K.clear_session()\n",
    "        self.generate_model(params)\n",
    "        self.compile_model(self.loss_function, hparams[self.optimizer])\n",
    "        return\n",
    "    \n",
    "    def eval_test(self, test):\n",
    "        _, acc = self.model.evaluate(test)\n",
    "        print(\"Accuracy on test set: \", acc)\n",
    "        return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultiLayerRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'optimizer': 'adam', 'num_units_l1': 16, 'num_units_l2': 64, 'num_units_l3': 30, 'dropout': 0.3}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 64)                6400      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                1950      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 532,221\n",
      "Trainable params: 532,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 4s 4s/step - loss: 3.1076 - accuracy: 0.5469WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.786792). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.786792). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    130/Unknown - 260s 2s/step - loss: 1.0343 - accuracy: 0.4887"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "points_explored, acc_params = m.random_search(train_dataset, validation_dataset, 42)\n",
    "randomized_search_time = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Points explored: \", points_explored)\n",
    "print(\"Accuracy for hparameters: \", acc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = sorted(acc_params, key=lambda x: x[0], reverse=True)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.setup_model(opt_params)\n",
    "m.eval_test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randomized_search_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f1331e3cd9b566b3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f1331e3cd9b566b3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir ./tf_logs/rnn_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "8.RNN-Text Classification Part II.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
