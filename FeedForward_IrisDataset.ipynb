{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeedForward_IrisDataset",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7NsoOUSBY-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qklUUOhC98Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iris_data_preprocessing():\n",
        "  # Preprocess the IRIS dataset\n",
        "  IRIS_Train = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "  IRIS_Test = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "  data_labels = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']\n",
        "  train_dataset = pd.read_csv(IRIS_Train, names=data_labels, skiprows=1)\n",
        "  test_dataset = pd.read_csv(IRIS_Test, names=data_labels, skiprows=1)\n",
        "\n",
        "  # Separate the X, y labels\n",
        "  y_train = pd.get_dummies(train_dataset.species)\n",
        "  X_train = train_dataset.drop('species', axis=1)\n",
        "\n",
        "  y_test = pd.get_dummies(test_dataset.species)\n",
        "  X_test = test_dataset.drop('species', axis=1)\n",
        "\n",
        "  print(\"Size of train set: \", X_train.shape)\n",
        "  print(\"Size of test set: \", X_test.shape)\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fopBd5PNBCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a512967f-3b8a-44b2-8b6d-ec58ca12d419"
      },
      "source": [
        "train, test = iris_data_preprocessing()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train set:  (120, 4)\n",
            "Size of test set:  (30, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hJ4RIB2G-NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardModel:\n",
        "\n",
        "  def __init__(self, learning_rate=0.005, num_iters=2000, batch_size=100):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.initialize_layers()\n",
        "    return\n",
        "  \n",
        "  def initialize_layers(self):\n",
        "    # TODO: Generalize this.\n",
        "\n",
        "    features = 4\n",
        "    neurons_h1 = 30   # Neurons in 1st hidden layer\n",
        "    neurons_h2 = 10   # Neurons in 2nd hidden layer\n",
        "    out_classes = 3\n",
        "\n",
        "    self.weights = {\n",
        "        \"h1\": tf.Variable(tf.random_normal([features, neurons_h1])),\n",
        "        \"h2\": tf.Variable(tf.random_normal([neurons_h1, neurons_h2])),\n",
        "        \"out\": tf.Variable(tf.random_normal([neurons_h2, out_classes]))\n",
        "    }\n",
        "\n",
        "    self.biases = {\n",
        "        \"b1\": tf.Variable(tf.random_normal([neurons_h1])),\n",
        "        \"b2\": tf.Variable(tf.random_normal([neurons_h2])),\n",
        "        \"out\": tf.Variable(tf.random_normal([out_classes])),\n",
        "    }\n",
        "    \n",
        "    self.X = tf.placeholder(\"float\", [None, features])\n",
        "    self.Y = tf.placeholder(\"float\", [None, out_classes])\n",
        "    return\n",
        "  \n",
        "  def create_model(self, x):\n",
        "    # Todo: Generalize model skeleton construction\n",
        "    hidden_layer_1 = tf.add(tf.matmul(x, self.weights[\"h1\"]), self.biases[\"b1\"])\n",
        "    hidden_layer_2 = tf.add(tf.matmul(hidden_layer_1, self.weights[\"h2\"]), self.biases[\"b2\"])\n",
        "    output_layer = tf.matmul(hidden_layer_2, self.weights[\"out\"]) + self.biases[\"out\"]\n",
        "    return output_layer\n",
        "    \n",
        "  def train_and_test(self, model, train_data, test_data):\n",
        "\n",
        "    X_train = train_data[0]\n",
        "    y_train = train_data[1]\n",
        "\n",
        "    X_test = test_data[0]\n",
        "    y_test = test_data[1]\n",
        "    \n",
        "\n",
        "    # Loss function. Todo: Generalize this.\n",
        "    loss_opn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=self.Y))\n",
        "    \n",
        "    # To minimize the loss\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
        "    train_proc = self.optimizer.minimize(loss_opn)\n",
        "    \n",
        "    # Eval\n",
        "    predict = tf.nn.softmax(model)\n",
        "    c_p = tf.equal(tf.argmax(predict, 1), tf.argmax(self.Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(c_p, tf.float32))\n",
        "    \n",
        "    init = tf.global_variables_initializer()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "\n",
        "    for i in range(self.num_iters):\n",
        "      sess.run(train_proc, feed_dict={self.X: X_train, self.Y: y_train})\n",
        "      if i % 500 == 0:\n",
        "        loss, acc = sess.run([loss_opn, accuracy], feed_dict={self.X: X_train, self.Y: y_train})\n",
        "        print(\"Iter \" + str(i) + \" Loss: \" + str(loss) + \" and Train accuracy: \" + str(acc))\n",
        "    \n",
        "    print(\"On Test Set:\")\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={self.X: X_test, self.Y: y_test}))\n",
        "    return\n",
        "\n",
        "  def run(self, train, test):\n",
        "    model = self.create_model(self.X)\n",
        "    self.train_and_test(model, train, test)\n",
        "    return\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJneKqrl8nM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c205e30d-cae0-4c30-aaa9-b80dec0da14e"
      },
      "source": [
        "nn = FeedForwardModel()\n",
        "nn.run(train, test)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0 Loss: 65.803764 and Train accuracy: 0.3\n",
            "Iter 500 Loss: 0.11276825 and Train accuracy: 0.96666664\n",
            "Iter 1000 Loss: 0.067073114 and Train accuracy: 0.975\n",
            "Iter 1500 Loss: 0.0666304 and Train accuracy: 0.975\n",
            "On Test Set:\n",
            "Accuracy:  0.96666664\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}