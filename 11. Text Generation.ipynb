{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Restrict TensorFlow to only use the second GPU\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    # The unique characters in the file\n",
    "    vocab = sorted(set(text))\n",
    "    print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "    # Print chars in vocab\n",
    "    print(\"Vocab: \", vocab)\n",
    "    \n",
    "    # VECTORIZE TEXT\n",
    "    # Mapping from unique characters to indices\n",
    "    char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "\n",
    "    print(\"Character to index map: \", char2idx)\n",
    "    print(\"Reverse map: \", idx2char)\n",
    "    \n",
    "    return vocab, char2idx, idx2char\n",
    "    \n",
    "\n",
    "def process_shakeaspeare_dataset():\n",
    "    # By default saves file to ~/.keras/datasets/fname. ~/.keras is the cache_dir and if file is already present there, then it is not downloaded again.\n",
    "    path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "    \n",
    "    # Read, then decode for py2 compat.\n",
    "    text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "    # length of text is the number of characters in it\n",
    "    print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "    # Peek at data\n",
    "    print(\"Sample text: \\n{} \\n\".format(text[:100]))\n",
    "    v, c2id, id2c = create_vocab(text)\n",
    "    return text, v, c2id, id2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "Sample text: \n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "65 unique characters\n",
      "Vocab:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Character to index map:  {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "Reverse map:  ['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "text, vocab, char2idx, idx2char = process_shakeaspeare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Task\n",
    "Given a sequence of characters, what is the most probable next character? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset.from_tensor_slices??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text \n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# create train data from the numpy array\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# We convert the individual chars to sequences of a desired size using batch method.\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# print above\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sequence, duplicate and shift it by one character to form the target text.\n",
    "# We use map method of the BatchDataset that applies a transformation to each element of the dataset and returns a new dataset containing transformed elements in the same order as input.\n",
    "# We need to pass the transformation function as the argument\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input data: \", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print(\"Target data: \", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# BUFFER_SIZE elements will be placed in a buffer and shuffled amongst themselves.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel:\n",
    "    def __init__(self, vocab, embedding_dim=256, rnn_units=1024, batch_size=BATCH_SIZE):\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.vocab = vocab\n",
    "        # Length of the vocabulary in chars\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # The embedding dimension\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Number of RNN units\n",
    "        self.rnn_units = rnn_units\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = self.get_optimizer()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.checkpoint_dir = './text_gen_train_checkpoints'\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt_{epoch}\")\n",
    "        return\n",
    "    \n",
    "    def get_optimizer(self):\n",
    "        return tf.keras.optimizers.Adam()\n",
    "    \n",
    "    def build_model(self, BATCH_SIZE):\n",
    "        self.model = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim, batch_input_shape=[BATCH_SIZE, None]), \n",
    "             tf.keras.layers.GRU(self.rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'), \n",
    "             tf.keras.layers.Dense(self.vocab_size)])\n",
    "        self.model.summary()\n",
    "        return self.model\n",
    "    \n",
    "    def get_entropy_loss(self, labels, logits):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    \n",
    "    def get_callbacks(self):   \n",
    "        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.checkpoint_prefix, save_weights_only=True)\n",
    "        return [ckpt_callback]\n",
    "    \n",
    "    # tf.function decorator ensures that this is still callable like a function and also compiled as a graph to leverage benefits as faster execution, exporting to SavedModel, and run on GPU/TPU.\n",
    "    @tf.function\n",
    "    def train_step(self, inp, target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(inp)\n",
    "            loss = tf.reduce_mean(self.get_entropy_loss(target, predictions))\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_model(self, dataset, EPOCHS=10):\n",
    "        for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "            \n",
    "            # initialize hidden state at the start of every epoch\n",
    "            hidden = self.model.reset_states()\n",
    "            \n",
    "            for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "                loss = self.train_step(inp, target)\n",
    "                \n",
    "                if batch_n % 100 == 0:\n",
    "                    template = \"Epoch {} Batch {} Loss {}\"\n",
    "                    print(template.format(epoch+1, batch_n, loss))\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.model.save_weights(self.checkpoint_prefix.format(epoch=epoch))\n",
    "            \n",
    "            print('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "            \n",
    "        self.model.save_weights(self.checkpoint_prefix.format(epoch=epoch))\n",
    "        return\n",
    "    \n",
    "    def generate_text(self, start_string, text_length):\n",
    "        \"\"\"\n",
    "        text_length: Number of characters to be generated\n",
    "        \"\"\"\n",
    "        input_eval = [char2idx[s] for s in start_string]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        \n",
    "        text_generated = []\n",
    "        \n",
    "        # Low temp results in more predictable text, and Higher temp results in more surprising text.\n",
    "        temprature = 1.0\n",
    "        \n",
    "        # TODO: Why?\n",
    "        self.model.reset_states()\n",
    "        \n",
    "        for i in range(text_length):\n",
    "            predictions = self.model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            \n",
    "            # using a categorical distribution to predict the word returned by the model\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            \n",
    "            # We pass the predicted word as the next input to the model along with the previous hidden state\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "            \n",
    "        return (start_string + ''.join(text_generated))\n",
    "    \n",
    "    def restore_model(self):\n",
    "        tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        self.model = self.build_model(batch_size=1)\n",
    "    \n",
    "    def test(self, dataset):\n",
    "        self.train_model(dataset)\n",
    "        #print(self.generate_text(u\"ROMEO: \", 1000))\n",
    "        return\n",
    "    \n",
    "    def test_generation(self):\n",
    "        self.model.build(tf.TensorShape([1, None]))\n",
    "        print(self.generate_text(u\"ROMEO: \", 1000))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1 Batch 0 Loss 4.1745924949646\n",
      "Epoch 1 Batch 100 Loss 2.3393023014068604\n",
      "Epoch 1 Loss 2.1580\n",
      "Time taken for 1 epoch 6.7360146045684814 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1526076793670654\n",
      "Epoch 2 Batch 100 Loss 1.945540428161621\n",
      "Epoch 2 Loss 1.8606\n",
      "Time taken for 1 epoch 5.560896396636963 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.8079391717910767\n",
      "Epoch 3 Batch 100 Loss 1.6943069696426392\n",
      "Epoch 3 Loss 1.6558\n",
      "Time taken for 1 epoch 5.532882213592529 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.6054534912109375\n",
      "Epoch 4 Batch 100 Loss 1.5394762754440308\n",
      "Epoch 4 Loss 1.5196\n",
      "Time taken for 1 epoch 5.876780271530151 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4252032041549683\n",
      "Epoch 5 Batch 100 Loss 1.5003193616867065\n",
      "Epoch 5 Loss 1.4570\n",
      "Time taken for 1 epoch 5.64849328994751 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.4075446128845215\n",
      "Epoch 6 Batch 100 Loss 1.4206396341323853\n",
      "Epoch 6 Loss 1.4170\n",
      "Time taken for 1 epoch 5.785002708435059 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3009347915649414\n",
      "Epoch 7 Batch 100 Loss 1.3499243259429932\n",
      "Epoch 7 Loss 1.3418\n",
      "Time taken for 1 epoch 5.7811598777771 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2623313665390015\n",
      "Epoch 8 Batch 100 Loss 1.2973583936691284\n",
      "Epoch 8 Loss 1.3407\n",
      "Time taken for 1 epoch 6.2297375202178955 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.2490980625152588\n",
      "Epoch 9 Batch 100 Loss 1.2608511447906494\n",
      "Epoch 9 Loss 1.2934\n",
      "Time taken for 1 epoch 5.855807542800903 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.206478238105774\n",
      "Epoch 10 Batch 100 Loss 1.26731538772583\n",
      "Epoch 10 Loss 1.2286\n",
      "Time taken for 1 epoch 6.125036954879761 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = CustomModel(vocab)\n",
    "cm.test(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftest",
   "language": "python",
   "name": "tftest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
