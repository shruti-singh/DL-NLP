{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Text Classification with RNN - IMDB Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDNWnKc3SrX",
        "colab_type": "text"
      },
      "source": [
        "## Text Classification with RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tehq8U213uSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade grpcio\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tfds-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvgtU7_c3Srd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYQNOi7VyJkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "00ece5de-d1aa-464a-df47-4153561189fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir_path = 'gdrive/My Drive/tensorflow_datasets/'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9BOFM7m3Srj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d19c2b17-6f0d-4d2a-fab7-ddbd5e6ded93"
      },
      "source": [
        "tfds.disable_progress_bar()\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True, data_dir=data_dir_path, download=False)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to gdrive/My Drive/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0...\u001b[0m\n",
            "\u001b[1mDataset imdb_reviews downloaded and prepared to gdrive/My Drive/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtqwRFDrAgBG",
        "colab_type": "text"
      },
      "source": [
        "***tfds.load()*** returns tf.data.Dataset and tf.core.DatasetInfo instances. \n",
        "\n",
        "[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) contains elements that have the same nested structure. The individual components of the structure can be of any type representable by [tf.TypeSpec](https://www.tensorflow.org/api_docs/python/tf/TypeSpec). tfds.load() returns all the splits (train, validation, and test) of the dataset if none is specified.\n",
        "\n",
        "[tf.core.DatasetInfo](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo) documents the dataset info like name, version, features, url, citations, metadata etc. tfds.load() returns the information of the entire dataset if with_info=True, irrespective of the split requested. Split specific information is available in the splits attribute of the instance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCwQuXaJ3Sro",
        "colab_type": "text"
      },
      "source": [
        "*tfds* includes a set of *TextEncoders* and *Tokenizers*.\n",
        "\n",
        "[**TextEncoder**](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/TextEncoder) class in TensorFlow: Is an abstract base class for conversion between integers and text. Since text data has variable length and requires padding, ID 0 is always reserved for padding.\n",
        "\n",
        "It has *vocab_size* as an attribute. vocab_size includes ID 0.\n",
        "\n",
        "Method *encode()* encodes text into a list of integers. It never returns ID 0, and all IDs are always 1+.\n",
        "Method *decode()* decodes a list of integers into text. It drops 0 in the input IDs.\n",
        "\n",
        "\n",
        "[**SubwordTextEncoder**](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder) is an invertible TextEncoder using word pieces with a byte-level fallback. This encoding is fully invertible as all out-of-vocab wordpieces are byte-encoded. \n",
        "\n",
        "It contains *vocab_list* attribute which contains a list of subwords for the vocabulary. An underscore at the end of the vocabulary indicates the end of the word. Underscores in the interior of subword are disallowed and should be used with escape sequence.\n",
        "\n",
        "**The dataset *info* includes the SubTextEncoder.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1uErvd-3Srp",
        "colab_type": "code",
        "outputId": "09b51596-5b35-4921-d004-8c3fc173df9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"info features: \", info.features)\n",
        "encoder = info.features[\"text\"].encoder\n",
        "print(\"\\n Vocabulary size: \", encoder.vocab_size)\n",
        "print(\"\\n Sample Subwords\", encoder.subwords[10:20])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info features:  FeaturesDict({\n",
            "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
            "})\n",
            "\n",
            " Vocabulary size:  8185\n",
            "\n",
            " Sample Subwords ['in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 'as_']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4kT9Tr3Srv",
        "colab_type": "text"
      },
      "source": [
        "As discussed above, the encoding is invertible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wIsYzFX3Srx",
        "colab_type": "code",
        "outputId": "8eaaa144-7b09-46bc-9f09-fa349e18c518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "sample_str = \"IMDB Review Classification\"\n",
        "print(\"Original string: \", sample_str)\n",
        "\n",
        "encoded_str = encoder.encode(sample_str)\n",
        "print(\"Encoded string is: \", encoded_str)\n",
        "\n",
        "decoded_str = encoder.decode(encoded_str)\n",
        "print(\"Decoded string si: \", decoded_str)\n",
        "\n",
        "for index in encoded_str:\n",
        "    print(\"%s ----> %s\"%(index, encoder.decode([index])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string:  IMDB Review Classification\n",
            "Encoded string is:  [5469, 7997, 2432, 3621, 739, 656, 2369, 1395, 3203, 757]\n",
            "Decoded string si:  IMDB Review Classification\n",
            "5469 ----> IM\n",
            "7997 ----> D\n",
            "2432 ----> B \n",
            "3621 ----> Rev\n",
            "739 ----> ie\n",
            "656 ----> w \n",
            "2369 ----> Cla\n",
            "1395 ----> ssi\n",
            "3203 ----> fic\n",
            "757 ----> ation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAcT7uU43Sr1",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the dataset for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xvM9kR53Sr3",
        "colab_type": "text"
      },
      "source": [
        "We create batches of the encoded strings. **padded_batch** is used to zero pad the sequences to the length of the longest sequence in the batch. It combines consecutive elements of the dataset into padded batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwIpfEJ43Sr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR9JvKSK3Sr9",
        "colab_type": "code",
        "outputId": "5bbdbc21-dfcb-4d6b-d37f-c48326ab2e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(dir(train_dataset))\n",
        "# fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "# output_shapes returns the shape of each component of an element of this dataset.\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
        "\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['_GeneratorState', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_add_variable_with_custom_getter', '_apply_options', '_as_serialized_graph', '_buffer_size', '_checkpoint_dependencies', '_component_metadata', '_consumers', '_deferred_dependencies', '_flat_shapes', '_flat_structure', '_flat_types', '_from_components', '_functions', '_gather_saveables_for_checkpoint', '_graph', '_graph_attr', '_handle_deferred_dependencies', '_has_captured_ref', '_input_dataset', '_inputs', '_is_graph_tensor', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_preload_simple_restoration', '_restore_from_checkpoint_position', '_self_name_based_restores', '_self_setattr_tracking', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_deferred_dependencies', '_self_unconditional_dependency_names', '_self_update_uid', '_setattr_tracking', '_shape_invariant_to_type_spec', '_single_restoration_from_checkpoint_position', '_tf_api_names', '_tf_api_names_v1', '_to_components', '_trace_variant_creation', '_track_trackable', '_tracking_metadata', '_type_spec', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', '_variant_tensor', '_variant_tensor_attr', '_variant_tracker', 'apply', 'batch', 'cache', 'concatenate', 'element_spec', 'enumerate', 'filter', 'flat_map', 'from_generator', 'from_tensor_slices', 'from_tensors', 'interleave', 'list_files', 'map', 'options', 'padded_batch', 'prefetch', 'range', 'reduce', 'repeat', 'shard', 'shuffle', 'skip', 'take', 'unbatch', 'window', 'with_options', 'zip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex9DsRcp3SsC",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2wZBTvMMINl",
        "colab_type": "text"
      },
      "source": [
        "**[Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)** inherits from the base class Model, and groups a linear stack of layers into an object with training and inference features. \n",
        "\n",
        "**[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)** inherits from the base class Layer and turns positive integer indices into vectors of fixed size. Takes as argument, input_dim (size of the vocabulary) and output_dim (dimension of the dense embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omvc9oqD3SsD",
        "colab_type": "code",
        "outputId": "c9d709e1-61db-4079-caf4-19eafe94e47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "n_hidden = 5\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=64))\n",
        "model.add(tf.keras.layers.SimpleRNN(units=n_hidden, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "# output_seqs, states = rnn.static_rnn(basic_cell, train_dataset, dtype=tf.float32)\n",
        "\n",
        "\n",
        "# TODO: Read about layers: tf.keras.layers.RNN, tf.keras.layers.LSTM, tf.keras.layers.GRU"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          523840    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 5)                 350       \n",
            "=================================================================\n",
            "Total params: 524,190\n",
            "Trainable params: 524,190\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eh2Bs2y3SsI",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/shruti-singh/DL-NLP/blob/master/images/SingleRNN.png?raw=1)"
      ]
    }
  ]
}