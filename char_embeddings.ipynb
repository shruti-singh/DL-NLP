{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard version:  2.0.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard import version;\n",
    "print(\"Tensorboard version: \", version.VERSION)\n",
    "#load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data(filename):\n",
    "    with open(filename) as f:\n",
    "        train = json.load(f)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_train_data(\"data/PaperQA-train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(data):\n",
    "    cleaned_data = []\n",
    "    for d in data['data']:\n",
    "        try:\n",
    "            title = d[\"Paper\"][\"Title\"].lower().strip().split()\n",
    "            abst = d[\"Paper\"][\"Abstract\"].lower().strip().split()\n",
    "            cl = \" \".join(title + abst)\n",
    "            cleaned_data.append(cl)\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = collate_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object categorization based on a supervised mean shift algorithm\n",
      "In this work, we present a C&#43;&#43; implementation of object categorization with the bag-of-word (BoW) framework. Unlike typical BoW models which consider the whole area of an image as the region of interest (ROI) for visual codebook generation, our implementation only considers the regions of target objects as ROIs and the unrelated backgrounds will be excluded for generating codebook. This is achieved by a supervised mean shift algorithm. Our work is on the benchmark SIVAL dataset and utilizes a Maximum Margin Supervised Topic Model for classification. The final performance of our work is quite encouraging.\n"
     ]
    }
   ],
   "source": [
    "print(train_data['data'][0][\"Paper\"][\"Title\"])\n",
    "print(train_data['data'][0][\"Paper\"][\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set(#papers):  1424\n",
      "sequence to sequence learning with neural networks deep neural networks (dnns) are powerful models that have achieved excellent performance on difficult learning tasks. although dnns work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. in this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. our method uses a multilayered long short-term memory (lstm) to map the input sequence to a vector of a fixed dimensionality, and then another deep lstm to decode the target sequence from the vector. our main result is that on an english to french translation task from the wmt-14 dataset, the translations produced by the lstm achieve a bleu score of 34.8 on the entire test set, where the lstm's bleu score was penalized on out-of-vocabulary words. additionally, the lstm did not have difficulty on long sentences. for comparison, a phrase-based smt system achieves a bleu score of 33.3 on the same dataset. when we used the lstm to rerank the 1000 hypotheses produced by the aforementioned smt system, its bleu score increases to 36.5, which is close to the previous state of the art. the lstm also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the lstm's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train set(#papers): \", len(cleaned_data))\n",
    "print(cleaned_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \" \".join(cleaned_data)\n",
    "vocab = sorted(set(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab:  69\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of vocab: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of title+abstract from 1424 papers:  1052.7991573033707\n"
     ]
    }
   ],
   "source": [
    "avg_len = 0\n",
    "for d in cleaned_data:\n",
    "    avg_len += len(d)\n",
    "print(\"Average length of title+abstract from 1424 papers: \", avg_len/len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences (of len: 200) are: 1216481\n"
     ]
    }
   ],
   "source": [
    "# Construct sequences\n",
    "seq_length = 200\n",
    "sequences = list()\n",
    "\n",
    "for d in cleaned_data:\n",
    "    for i in range(seq_length, len(d)):\n",
    "        seq = d[i-seq_length: i+1]\n",
    "        sequences.append(seq)\n",
    "\n",
    "print(\"Total sequences (of len: %d) are: %d\"%(seq_length, len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequences\n",
    "en_sequences = list()\n",
    "for line in sequences:\n",
    "    # encode via integer\n",
    "    encoded_seq = [char2idx[char] for char in line]\n",
    "    en_sequences.append(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequences):\n",
    "    s = np.array(sequences)\n",
    "    X, y = s[:,:-1], s[:, -1]\n",
    "    \n",
    "    # reduce size\n",
    "#     X = X[:50000]\n",
    "#     y = y[:50000]\n",
    "    return X, y\n",
    "\n",
    "X, y = split_input_target(en_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X[:50000], y[:50000]))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X[60000:70000], y[60000:70000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X[80000:90000], y[80000:90000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidir_layer = Bidirectional(LSTM(1000))\n",
    "# pen_dense_layer = Dense(69, activation='softmax')\n",
    "# final_dense_layer = Dense(100, activation='softmax')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(bidir_layer)\n",
    "# model.add(final_dense_layer)\n",
    "# # print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/10\n",
      "   32/50000 [..............................] - ETA: 51:36"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[32,100] labels_size=[32,69]\n\t [[node loss/output_1_loss/softmax_cross_entropy_with_logits (defined at /home/singh_shruti/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_12375]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-910fa33e4e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/workspace/tftest/lib64/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[32,100] labels_size=[32,69]\n\t [[node loss/output_1_loss/softmax_cross_entropy_with_logits (defined at /home/singh_shruti/workspace/tftest/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_12375]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Bidirectional??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLayerRNN:\n",
    "    \n",
    "#     def __init__(self, teacher_forcing=False):\n",
    "#         # number of units in 1st and 2nd recurrent layer, and the next dense layer\n",
    "#         self.num_units_l1 = hp.HParam('num_units_l1', hp.Discrete([64, 128, 256]))\n",
    "#         self.num_units_l2 = hp.HParam('num_units_l2', hp.Discrete([64, 128, 256]))\n",
    "#         #self.dropout = hp.HParam('dropout', hp.Discrete([0.3, 0.4]))\n",
    "\n",
    "#         self.optimizer = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "        \n",
    "#         self.hparams = {self.optimizer: self.optimizer, self.num_units_l1: self.num_units_l1, \n",
    "#                         self.num_units_l2: self.num_units_l2}\n",
    "#                         #self.dropout: self.dropout}\n",
    "        \n",
    "#         self.teacher_forcing = teacher_forcing\n",
    "        \n",
    "#         self.model = None\n",
    "        \n",
    "#         METRIC_ACCURACY = 'accuracy'\n",
    "        \n",
    "#         self.init_timestamp = int(time())\n",
    "        \n",
    "#         print(\"MODEL INIT TIMESTAMP: \", str(self.init_timestamp))\n",
    "        \n",
    "#         self.log_dir = \"./tf_logs/char_emb_\" + str(self.init_timestamp) +\"/\"\n",
    "#         with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "#             #self.dropout\n",
    "#             hp.hparams_config(hparams=[self.optimizer, self.num_units_l1, self.num_units_l2], \n",
    "#                               metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def loss_function(self, y_true, y_pred):\n",
    "#         return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "#     def generate_model(self, params):\n",
    "        \n",
    "#         self.model = tf.keras.Sequential()\n",
    "#         self.model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=64))\n",
    "#         self.model.add(tf.keras.layers.RNN([CustomRNNCell(params[self.num_units_l1]), \n",
    "#                                             CustomRNNCell(params[self.num_units_l2])]))\n",
    "#         self.model.add(tf.keras.layers.Dropout(params[self.dropout]))\n",
    "#         self.model.add(tf.keras.layers.Dense(params[self.num_units_l3], activation='relu'))\n",
    "#         self.model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "#         print(self.model.summary())\n",
    "#         return self.model\n",
    "    \n",
    "#     def get_model(self):\n",
    "#         return self.model\n",
    "    \n",
    "#     def save_model(self):\n",
    "#         cp = time()\n",
    "#         model.save_weights(self.logdir + '/saved_models/model_' + cp, save_format='tf')\n",
    "#         return\n",
    "    \n",
    "#     def compile_model(self, loss_function, optimizer):\n",
    "#         self.model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "#         return self.model\n",
    "    \n",
    "#     def train_model(self, hparams, train_data, cross_validation_data, run_index):\n",
    "#         self.generate_model(hparams)\n",
    "#         self.compile_model(self.loss_function, hparams[self.optimizer])\n",
    "#         #self.compile_model(self.loss_function, self.optimizer)\n",
    "        \n",
    "#         callbacks = [\n",
    "#             # Early stopping\n",
    "#             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4),\n",
    "#             # logging train and validation accuracy each epoch\n",
    "#             tf.keras.callbacks.TensorBoard(log_dir=self.log_dir + run_index),\n",
    "#             # gradient sum callback\n",
    "#             gradient_cb,\n",
    "#             # checkpoint\n",
    "#             #tf.keras.callbacks.ModelCheckpoint(filepath=, save_weights_only=True, save_best_only=True)\n",
    "#         ]\n",
    "        \n",
    "#         self.model.fit(train_data, epochs=10, validation_data=cross_validation_data, callbacks=callbacks, \n",
    "#                        verbose=1)\n",
    "#         _, accuracy = self.model.evaluate(cross_validation_data)\n",
    "        \n",
    "#         return accuracy\n",
    "    \n",
    "#     def run(self, run_dir, hparams, train_data, cross_validation_data):\n",
    "#         K.clear_session()\n",
    "#         run_index = run_dir.split(\"-\")[1]\n",
    "#         with tf.summary.create_file_writer(run_dir).as_default():\n",
    "#             # record the values used in this trial\n",
    "#             hp.hparams(hparams)\n",
    "#             acc = self.train_model(hparams, train_dataset, validation_dataset, run_index)\n",
    "#             tf.summary.scalar('accuracy', acc, step=int(run_index))\n",
    "#         return acc\n",
    "    \n",
    "#     def random_search(self, train, cross_val, seed):\n",
    "#         rng = random.Random(seed)\n",
    "#         total_points_explored = 1\n",
    "        \n",
    "#         acc_params = []\n",
    "        \n",
    "#         for session_index in range(total_points_explored):\n",
    "#             hparams = {h: h.domain.sample_uniform(rng) for h in self.hparams}\n",
    "#             run_name = \"run-%d\" % session_index\n",
    "#             print('--- Starting trial: %s' % run_name)\n",
    "#             print({h.name: hparams[h] for h in hparams})\n",
    "#             acc = self.run(self.log_dir + \"tune/\" + run_name, hparams, train, cross_val)\n",
    "#             session_index += 1\n",
    "#             acc_params.append((acc, hparams))\n",
    "        \n",
    "#         return total_points_explored, acc_params\n",
    "    \n",
    "#     def setup_model(self, params):\n",
    "#         K.clear_session()\n",
    "#         self.generate_model(params)\n",
    "#         self.compile_model(self.loss_function, params[self.optimizer])\n",
    "#         return\n",
    "    \n",
    "#     def test_params(self, hpa):\n",
    "#         K.clear_session()\n",
    "#         params = {h: hpa[h.name] for h in self.hparams}\n",
    "#         self.generate_model(params)\n",
    "#         self.compile_model(self.loss_function, params[self.optimizer])\n",
    "#         self.run(self.log_dir + \"tune/testhpgrad-1\", params, train_dataset, validation_dataset)\n",
    "#         return\n",
    "    \n",
    "#     def eval_test(self, test):\n",
    "#         _, acc = self.model.evaluate(test)\n",
    "#         print(\"Accuracy on test set: \", acc)\n",
    "#         return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "# output_shapes returns the shape of each component of an element of this dataset.\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "\n",
    "val_dataset = val_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(val_dataset))\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerLSTM:\n",
    "    \n",
    "    def __init__(self, teacher_forcing=False):\n",
    "        # number of units in 1st and 2nd LSTM layer, and the next dense layer\n",
    "        self.num_units_l1 = hp.HParam('num_units_l1', hp.Discrete([64]))\n",
    "        self.num_units_l2 = hp.HParam('num_units_l2', hp.Discrete([64]))\n",
    "        self.num_units_l3 = hp.HParam('num_units_l3', hp.Discrete([100]))\n",
    "        self.optimizer = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "        \n",
    "        self.hparams = {self.optimizer: self.optimizer, self.num_units_l1: self.num_units_l1, self.num_units_l2: self.num_units_l2, self.num_units_l3: self.num_units_l3}\n",
    "        \n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        METRIC_ACCURACY = 'accuracy'\n",
    "        \n",
    "        self.timestamp = int(time())\n",
    "        print(\"MODEL INIT TIME: \", str(self.timestamp))\n",
    "        self.log_dir = \"./tf_logs/char_emb_\" + str(self.timestamp) +\"/\"\n",
    "        with tf.summary.create_file_writer(self.log_dir).as_default():\n",
    "            hp.hparams_config(hparams=[self.optimizer, self.num_units_l1, self.num_units_l2, self.num_units_l3], metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def loss_function(self, y_true, y_pred):\n",
    "        return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    def generate_model(self, params, single_layer=True):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64))\n",
    "        self.model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params[self.num_units_l1], return_sequences=True)))\n",
    "        self.model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params[self.num_units_l2])))\n",
    "        self.model.add(tf.keras.layers.Dense(params[self.num_units_l3], activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "        print(self.model.summary())\n",
    "        return self.model\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self):\n",
    "        cp = int(time())\n",
    "        model.save_weights(self.logdir + '/saved_models/model_' + cp, save_format='tf')\n",
    "        return\n",
    "    \n",
    "    def compile_model(self, loss_function, optimizer):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "        return self.model\n",
    "    \n",
    "    def train_model(self, hparams, train_data, cross_validation_data, run_index):\n",
    "        self.generate_model(hparams)\n",
    "        self.compile_model(self.loss_function, hparams[self.optimizer])\n",
    "        \n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=self.log_dir + run_index),\n",
    "        ]\n",
    "        \n",
    "        self.model.fit(train_data, epochs=3, validation_data=cross_validation_data, callbacks=callbacks,verbose=1)\n",
    "        _, accuracy = self.model.evaluate(cross_validation_data)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def run(self, run_dir, hparams, train_data, cross_validation_data):\n",
    "        K.clear_session()\n",
    "        run_index = run_dir.split(\"-\")[1]\n",
    "        with tf.summary.create_file_writer(run_dir).as_default():\n",
    "            # record the values used in this trial\n",
    "            hp.hparams(hparams)\n",
    "            acc = self.train_model(hparams, train_dataset, cross_validation_data, run_index)\n",
    "            tf.summary.scalar('accuracy', acc, step=int(run_index))\n",
    "        return acc\n",
    "    \n",
    "    def random_search(self, train, cross_val, seed):\n",
    "        rng = random.Random(seed)\n",
    "        total_points_explored = 1\n",
    "        \n",
    "        acc_params = []\n",
    "        \n",
    "        for session_index in range(total_points_explored):\n",
    "            hparams = {h: h.domain.sample_uniform(rng) for h in self.hparams}\n",
    "            run_name = \"run-%d\" % session_index\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            acc = self.run(self.log_dir + \"tune/\" + run_name, hparams, train, cross_val)\n",
    "            session_index += 1\n",
    "            acc_params.append((acc, hparams))\n",
    "\n",
    "        #Todo: call setup here only.\n",
    "        \n",
    "        return total_points_explored, acc_params\n",
    "    \n",
    "    def setup_model(self, hparams, single_layer=True):\n",
    "        self.generate_model(hparams, single_layer)\n",
    "        self.compile_model(self.loss_function, hparams[self.optimizer])\n",
    "        return self.model\n",
    "    \n",
    "    def eval_test(self, test):\n",
    "        _, acc = self.model.evaluate(test)\n",
    "        print(\"Acc on tst set: \")\n",
    "        print(acc)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INIT TIME:  1581394047\n",
      "--- Starting trial: run-0\n",
      "{'optimizer': 'adam', 'num_units_l1': 64, 'num_units_l2': 64, 'num_units_l3': 100}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          4416      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 182,281\n",
      "Trainable params: 182,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "      1/Unknown - 8s 8s/step - loss: 0.6011 - accuracy: 0.0469"
     ]
    }
   ],
   "source": [
    "m = MultiLayerLSTM()\n",
    "points_explored, acc_params = m.random_search(train_dataset, val_dataset, 42)\n",
    "opt_params = sorted(acc_params,key=lambda x: x[0], reverse=True)[0][1]\n",
    "m.setup_model(opt_params)\n",
    "m.eval_test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftest",
   "language": "python",
   "name": "tftest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
